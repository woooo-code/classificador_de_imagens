{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/media/veracrypt6/flower-classifier-master/\"\n",
    "train_dir = os.path.join(root_dir, \"treino\")\n",
    "val_dir = os.path.join(root_dir, \"valida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Number of training images per category:')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "category_names = sorted(os.listdir(train_dir))\n",
    "nb_categories = len(category_names)\n",
    "img_pr_cat = []\n",
    "for category in category_names:\n",
    "    folder = train_dir + '/' + category\n",
    "    img_pr_cat.append(len(os.listdir(folder)))\n",
    "\n",
    "sns.barplot(y=category_names, x=img_pr_cat).set_title(\"Number of training images per category:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Xception model from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224) + (3,)\n",
    "base_model = keras.applications.Xception(include_top=False,\n",
    "                                         weights=\"imagenet\", input_shape=IMG_SIZE, pooling=\"avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = keras.models.Sequential()\n",
    "my_model.add(base_model)\n",
    "my_model.add(keras.layers.Dense(nb_categories, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "train_data_generator = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 247 images belonging to 2 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data_generator.flow_from_directory(train_dir, target_size=(224, 224),\n",
    "                                              class_mode=\"categorical\", batch_size=12)\n",
    "val_data = val_data_generator.flow_from_directory(val_dir, target_size=(224, 224),\n",
    "                                              class_mode=\"categorical\", batch_size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3347 - accuracy: 0.8381\n",
      "Epoch 00001: saving model to /media/flower-classifier-master/training_1/cp.ckpt\n",
      "21/21 [==============================] - 36s 2s/step - loss: 0.3347 - accuracy: 0.8381\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9919\n",
      "Epoch 00002: saving model to /media/flower-classifier-master/training_1/cp.ckpt\n",
      "21/21 [==============================] - 37s 2s/step - loss: 0.0698 - accuracy: 0.9919\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9960\n",
      "Epoch 00003: saving model to /media/flower-classifier-master/training_1/cp.ckpt\n",
      "21/21 [==============================] - 37s 2s/step - loss: 0.0377 - accuracy: 0.9960\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 00004: saving model to /media/flower-classifier-master/training_1/cp.ckpt\n",
      "21/21 [==============================] - 36s 2s/step - loss: 0.0259 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 00005: saving model to /media/flower-classifier-master/training_1/cp.ckpt\n",
      "21/21 [==============================] - 36s 2s/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 00006: saving model to /media/flower-classifier-master/training_1/cp.ckpt\n",
      "21/21 [==============================] - 36s 2s/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 00007: saving model to /media/flower-classifier-master/training_1/cp.ckpt\n",
      "21/21 [==============================] - 38s 2s/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 00008: saving model to /media/flower-classifier-master/training_1/cp.ckpt\n",
      "21/21 [==============================] - 37s 2s/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 00009: saving model to /media/flower-classifier-master/training_1/cp.ckpt\n",
      "21/21 [==============================] - 36s 2s/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 00010: saving model to /media/flower-classifier-master/training_1/cp.ckpt\n",
      "21/21 [==============================] - 37s 2s/step - loss: 0.0080 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/media/flower-classifier-master/training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Train the model with the new callback\n",
    "history = my_model.fit(train_data, epochs=12, validation_data=val_data, callbacks=[cp_callback])  # Pass callback to training\n",
    "\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = my_model.fit(train_data, epochs=10, validation_data=val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model=my_model, filepath=\"flower_classifier.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto the web app!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
